{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4407413",
   "metadata": {},
   "source": [
    "# Ensemble Methods — Random Forests\n",
    "\n",
    "This notebook demonstrates **bagging** and **random forests** using our NumPy-only implementations in `rice_ml.supervised_learning.ensemble_methods`.\n",
    "\n",
    "We focus on two ideas:\n",
    "- **Bagging reduces variance** by averaging many high-variance learners (decision trees).\n",
    "- **Random feature subsampling** (`max_features`) decorrelates trees, improving the ensemble.\n",
    "\n",
    "We use two classification datasets (Iris 2D and Breast Cancer) and two regression datasets (Diabetes and a 1D synthetic example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f453ef25",
   "metadata": {},
   "source": [
    "## 1. Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43952da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def add_repo_src_to_path(max_up: int = 8) -> None:\n",
    "    cur = os.path.abspath(os.getcwd())\n",
    "    for _ in range(max_up):\n",
    "        candidate = os.path.join(cur, \"src\")\n",
    "        if os.path.isdir(os.path.join(candidate, \"rice_ml\")):\n",
    "            if candidate not in sys.path:\n",
    "                sys.path.insert(0, candidate)\n",
    "            return\n",
    "        cur = os.path.abspath(os.path.join(cur, \"..\"))\n",
    "    raise RuntimeError(\"Could not find 'src/rice_ml'. Run this notebook inside the repo, or install the package.\")\n",
    "\n",
    "add_repo_src_to_path()\n",
    "\n",
    "from rice_ml.processing.preprocessing import standardize, train_test_split\n",
    "from rice_ml.supervised_learning.decision_trees import DecisionTreeClassifier\n",
    "from rice_ml.supervised_learning.regression_trees import RegressionTreeRegressor\n",
    "from rice_ml.supervised_learning.ensemble_methods import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, load_diabetes\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f79387c",
   "metadata": {},
   "source": [
    "## 2. Helper metrics + plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410014db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred) -> float:\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return float(np.mean(y_true == y_pred))\n",
    "\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "def r2_score(y_true, y_pred) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    if ss_tot == 0:\n",
    "        return 0.0\n",
    "    return float(1.0 - ss_res / ss_tot)\n",
    "\n",
    "def confusion_matrix_np(y_true, y_pred, labels=None):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    if labels is None:\n",
    "        labels = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    labels = np.asarray(labels)\n",
    "    k = labels.size\n",
    "    cm = np.zeros((k, k), dtype=int)\n",
    "    for i, a in enumerate(labels):\n",
    "        for j, b in enumerate(labels):\n",
    "            cm[i, j] = int(np.sum((y_true == a) & (y_pred == b)))\n",
    "    return cm, labels\n",
    "\n",
    "def plot_decision_regions_2d(\n",
    "    X_raw,\n",
    "    y,\n",
    "    clf,\n",
    "    title: str,\n",
    "    xlabel: str,\n",
    "    ylabel: str,\n",
    "    mean=None,\n",
    "    scale=None,\n",
    "    h: float = 0.02,\n",
    "):\n",
    "    \"\"\"Plot decision regions in the *raw* 2D feature space.\n",
    "\n",
    "    If `mean` and `scale` are provided, the grid points are standardized\n",
    "    before being passed to the classifier (useful when the model was fit\n",
    "    on standardized features, but we want axes in original units).\n",
    "    \"\"\"\n",
    "    X_raw = np.asarray(X_raw)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    x_min, x_max = X_raw[:, 0].min() - 0.6, X_raw[:, 0].max() + 0.6\n",
    "    y_min, y_max = X_raw[:, 1].min() - 0.6, X_raw[:, 1].max() + 0.6\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    grid_raw = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    grid = grid_raw\n",
    "    if mean is not None and scale is not None:\n",
    "        grid = (grid_raw - mean) / scale\n",
    "\n",
    "    Z = clf.predict(grid).reshape(xx.shape)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.contourf(xx, yy, Z, alpha=0.25)\n",
    "    plt.scatter(X_raw[:, 0], X_raw[:, 1], c=y, edgecolor=\"k\", s=25)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618b33a2",
   "metadata": {},
   "source": [
    "## 3. Part A — Classification (Iris, 2D)\n",
    "\n",
    "We use **petal length** and **petal width** (2D) to visualize decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1955e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X2 = iris.data[:, [2, 3]]  # petal length, petal width\n",
    "y = iris.target\n",
    "\n",
    "print(\"X2 shape:\", X2.shape)\n",
    "print(\"y shape :\", y.shape)\n",
    "print(\"classes :\", list(enumerate(iris.target_names)))\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(\"class counts:\", dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4260da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X2[:, 0], X2[:, 1], c=y, edgecolor=\"k\", s=25)\n",
    "plt.xlabel(\"petal length (cm)\")\n",
    "plt.ylabel(\"petal width (cm)\")\n",
    "plt.title(\"Iris (raw, 2D features)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c926e4",
   "metadata": {},
   "source": [
    "### 3.1 Train/test split + standardization\n",
    "\n",
    "Trees/forests do **not** require feature scaling, but we keep a consistent preprocessing workflow and later verify that scaling does not change predictions (with fixed seeds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546d6c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X2, y,\n",
    "    test_size=0.25,\n",
    "    shuffle=True,\n",
    "    stratify=y,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_train_std, params = standardize(X_train, return_params=True)\n",
    "mean = params[\"mean\"]\n",
    "scale = params[\"scale\"]\n",
    "X_test_std = (X_test - mean) / scale\n",
    "\n",
    "print(\"Train mean (approx):\", np.round(X_train_std.mean(axis=0), 6))\n",
    "print(\"Train std  (approx):\", np.round(X_train_std.std(axis=0), 6))\n",
    "print(\"Test  mean (approx):\", np.round(X_test_std.mean(axis=0), 6))\n",
    "print(\"Test  std  (approx):\", np.round(X_test_std.std(axis=0), 6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6320f16d",
   "metadata": {},
   "source": [
    "### 3.2 Fit a Random Forest and visualize decision regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca74dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=3,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    ").fit(X_train_std, y_train)\n",
    "\n",
    "train_acc = accuracy(y_train, rf.predict(X_train_std))\n",
    "test_acc = accuracy(y_test, rf.predict(X_test_std))\n",
    "\n",
    "print(\"RandomForestClassifier (Iris 2D) | max_depth=3\")\n",
    "print(\"  train acc:\", train_acc)\n",
    "print(\"  test  acc:\", test_acc)\n",
    "\n",
    "y_pred = rf.predict(X_test_std)\n",
    "cm, labels = confusion_matrix_np(y_test, y_pred, labels=[0, 1, 2])\n",
    "print(\"\\nConfusion matrix (test): rows=true, cols=pred, labels=[0,1,2]\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535b769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions_2d(\n",
    "    X_train, y_train, rf,\n",
    "    title=\"Decision regions (train) — Iris 2D | Random Forest | max_depth=3\",\n",
    "    xlabel=\"petal length (cm)\",\n",
    "    ylabel=\"petal width (cm)\",\n",
    "    mean=mean,\n",
    "    scale=scale,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9156653a",
   "metadata": {},
   "source": [
    "### 3.3 Scaling check: raw vs standardized\n",
    "\n",
    "We fit the same forest twice (same `random_state`) on raw vs standardized features and compare predictions on the **test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a28930",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_raw = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=3,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "rf_std = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=3,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    ").fit(X_train_std, y_train)\n",
    "\n",
    "pred_raw = rf_raw.predict(X_test)\n",
    "pred_std = rf_std.predict(X_test_std)\n",
    "\n",
    "print(\"Test accuracy (raw features):\", accuracy(y_test, pred_raw))\n",
    "print(\"Test accuracy (standardized):\", accuracy(y_test, pred_std))\n",
    "print(\"Predictions identical?      :\", bool(np.array_equal(pred_raw, pred_std)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a555c91",
   "metadata": {},
   "source": [
    "### 3.4 Hyperparameter sweeps (Iris 2D)\n",
    "\n",
    "We scan `max_depth` and `n_estimators` to see the bias–variance pattern in classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b11db",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = list(range(1, 11))\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "for d in depths:\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=d,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        random_state=42,\n",
    "    ).fit(X_train_std, y_train)\n",
    "    train_accs.append(accuracy(y_train, clf.predict(X_train_std)))\n",
    "    test_accs.append(accuracy(y_test, clf.predict(X_test_std)))\n",
    "\n",
    "best_d = depths[int(np.argmax(test_accs))]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(depths, train_accs, marker=\"o\", label=\"train accuracy\")\n",
    "plt.plot(depths, test_accs, marker=\"o\", label=\"test accuracy\")\n",
    "plt.axvline(best_d, linestyle=\"--\", label=f\"best depth = {best_d}\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Iris 2D — Accuracy vs max_depth (Random Forest)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Best depth by test accuracy:\", best_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263fa048",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_list = [1, 5, 10, 25, 50, 100, 200, 400]\n",
    "train_accs_n = []\n",
    "test_accs_n = []\n",
    "\n",
    "for n_estimators in n_list:\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=best_d,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        random_state=42,\n",
    "    ).fit(X_train_std, y_train)\n",
    "    train_accs_n.append(accuracy(y_train, clf.predict(X_train_std)))\n",
    "    test_accs_n.append(accuracy(y_test, clf.predict(X_test_std)))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(n_list, train_accs_n, marker=\"o\", label=\"train accuracy\")\n",
    "plt.plot(n_list, test_accs_n, marker=\"o\", label=\"test accuracy\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(f\"Iris 2D — Accuracy vs n_estimators (max_depth={best_d})\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9b212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = [\n",
    "    (\"all (None)\", None),\n",
    "    (\"sqrt(p)\", \"sqrt\"),\n",
    "    (\"0.3*p\", 0.3),\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for name, mf in settings:\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=best_d,\n",
    "        max_features=mf,\n",
    "        bootstrap=True,\n",
    "        random_state=42,\n",
    "    ).fit(X_train_std, y_train)\n",
    "    rows.append([\n",
    "        name,\n",
    "        mf,\n",
    "        accuracy(y_train, clf.predict(X_train_std)),\n",
    "        accuracy(y_test, clf.predict(X_test_std)),\n",
    "    ])\n",
    "\n",
    "print(\"setting\\tmax_features\\ttrain_acc\\ttest_acc\")\n",
    "for r in rows:\n",
    "    print(f\"{r[0]}\\t{r[1]}\\t{r[2]:.4f}\\t{r[3]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b7b93f",
   "metadata": {},
   "source": [
    "## 4. Part B — Classification (Breast Cancer, 30D)\n",
    "\n",
    "We compare a **single decision tree** vs a **random forest** on a higher-dimensional dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target  # {0,1}\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"classes:\", list(enumerate(data.target_names)))\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(\"class counts:\", dict(zip(unique, counts)))\n",
    "print(\"Any NaN in X?\", bool(np.isnan(X).any()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55409e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.25,\n",
    "    shuffle=True,\n",
    "    stratify=y,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_train_std, params = standardize(X_train, return_params=True)\n",
    "mean = params[\"mean\"]\n",
    "scale = params[\"scale\"]\n",
    "X_test_std = (X_test - mean) / scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d51db",
   "metadata": {},
   "source": [
    "### 4.1 Single tree vs forest (same depth)\n",
    "\n",
    "A forest should reduce variance relative to a single high-variance tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5333cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=5, random_state=42).fit(X_train_std, y_train)\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=5,\n",
    "    max_features=\"sqrt\",\n",
    "    random_state=42,\n",
    ").fit(X_train_std, y_train)\n",
    "\n",
    "print(\"DecisionTreeClassifier | max_depth=5\")\n",
    "print(\"  train acc:\", accuracy(y_train, tree.predict(X_train_std)))\n",
    "print(\"  test  acc:\", accuracy(y_test, tree.predict(X_test_std)))\n",
    "\n",
    "print(\"\\nRandomForestClassifier | n_estimators=300, max_depth=5, max_features='sqrt'\")\n",
    "print(\"  train acc:\", accuracy(y_train, rf.predict(X_train_std)))\n",
    "print(\"  test  acc:\", accuracy(y_test, rf.predict(X_test_std)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761955ec",
   "metadata": {},
   "source": [
    "### 4.2 Sweep `max_depth` (forest)\n",
    "\n",
    "We scan depth and see where test accuracy stabilizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d8a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = list(range(1, 16))\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "for d in depths:\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=d,\n",
    "        max_features=\"sqrt\",\n",
    "        random_state=42,\n",
    "    ).fit(X_train_std, y_train)\n",
    "    train_accs.append(accuracy(y_train, clf.predict(X_train_std)))\n",
    "    test_accs.append(accuracy(y_test, clf.predict(X_test_std)))\n",
    "\n",
    "best_d_bc = depths[int(np.argmax(test_accs))]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(depths, train_accs, marker=\"o\", label=\"train accuracy\")\n",
    "plt.plot(depths, test_accs, marker=\"o\", label=\"test accuracy\")\n",
    "plt.axvline(best_d_bc, linestyle=\"--\", label=f\"best depth = {best_d_bc}\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Breast Cancer — Accuracy vs max_depth (Random Forest)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Best depth by test accuracy:\", best_d_bc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ffaefa",
   "metadata": {},
   "source": [
    "### 4.3 Sweep `n_estimators` (forest)\n",
    "\n",
    "More trees typically reduce variance and stabilize test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ddf0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_list = [1, 5, 10, 25, 50, 100, 200, 400]\n",
    "train_accs_n = []\n",
    "test_accs_n = []\n",
    "\n",
    "for n_estimators in n_list:\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=best_d_bc,\n",
    "        max_features=\"sqrt\",\n",
    "        random_state=42,\n",
    "    ).fit(X_train_std, y_train)\n",
    "    train_accs_n.append(accuracy(y_train, clf.predict(X_train_std)))\n",
    "    test_accs_n.append(accuracy(y_test, clf.predict(X_test_std)))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(n_list, train_accs_n, marker=\"o\", label=\"train accuracy\")\n",
    "plt.plot(n_list, test_accs_n, marker=\"o\", label=\"test accuracy\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(f\"Breast Cancer — Accuracy vs n_estimators (max_depth={best_d_bc})\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e259c80f",
   "metadata": {},
   "source": [
    "### 4.4 Compare `max_features` settings (forest)\n",
    "\n",
    "Forests often benefit from restricting features per split (decorrelation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82533713",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = [\n",
    "    (\"all (None)\", None),\n",
    "    (\"sqrt(p)\", \"sqrt\"),\n",
    "    (\"log2(p)\", \"log2\"),\n",
    "    (\"0.3*p\", 0.3),\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for name, mf in settings:\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=best_d_bc,\n",
    "        max_features=mf,\n",
    "        random_state=42,\n",
    "    ).fit(X_train_std, y_train)\n",
    "    rows.append([\n",
    "        name,\n",
    "        mf,\n",
    "        accuracy(y_train, clf.predict(X_train_std)),\n",
    "        accuracy(y_test, clf.predict(X_test_std)),\n",
    "    ])\n",
    "\n",
    "print(\"setting\\tmax_features\\ttrain_acc\\ttest_acc\")\n",
    "for r in rows:\n",
    "    print(f\"{r[0]}\\t{r[1]}\\t{r[2]:.4f}\\t{r[3]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c7b63",
   "metadata": {},
   "source": [
    "## 5. Part C — Regression (Diabetes)\n",
    "\n",
    "We compare a single regression tree vs a random forest regressor using **RMSE** and **R²**.\n",
    "\n",
    "To keep runtimes reasonable and match earlier notebooks, we use **5 features** (the first 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7e2548",
   "metadata": {},
   "outputs": [],
   "source": [
    "diab = load_diabetes()\n",
    "X = diab.data[:, :5]  # 5 features\n",
    "y = diab.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.25,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_train_std, params = standardize(X_train, return_params=True)\n",
    "mean = params[\"mean\"]\n",
    "scale = params[\"scale\"]\n",
    "X_test_std = (X_test - mean) / scale\n",
    "\n",
    "print(\"Train mean (approx):\", np.round(X_train_std.mean(axis=0), 4))\n",
    "print(\"Train std  (approx):\", np.round(X_train_std.std(axis=0), 4))\n",
    "print(\"Test  mean (approx):\", np.round(X_test_std.mean(axis=0), 4))\n",
    "print(\"Test  std  (approx):\", np.round(X_test_std.std(axis=0), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa055a7d",
   "metadata": {},
   "source": [
    "### 5.1 Baselines: mean predictor, single tree, random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da1158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean baseline (predict training mean)\n",
    "baseline_pred = np.full_like(y_test, float(np.mean(y_train)))\n",
    "print(\"Mean baseline\")\n",
    "print(\"  test  RMSE:\", rmse(y_test, baseline_pred))\n",
    "print(\"  test  R^2 :\", r2_score(y_test, baseline_pred))\n",
    "\n",
    "# Single regression tree\n",
    "tree = RegressionTreeRegressor(max_depth=5, min_samples_leaf=5, random_state=42).fit(X_train_std, y_train)\n",
    "pred_tree_train = tree.predict(X_train_std)\n",
    "pred_tree_test = tree.predict(X_test_std)\n",
    "\n",
    "print(\"\\nRegressionTreeRegressor | max_depth=5, min_samples_leaf=5\")\n",
    "print(\"  train RMSE:\", rmse(y_train, pred_tree_train))\n",
    "print(\"  test  RMSE:\", rmse(y_test, pred_tree_test))\n",
    "print(\"  train R^2 :\", r2_score(y_train, pred_tree_train))\n",
    "print(\"  test  R^2 :\", r2_score(y_test, pred_tree_test))\n",
    "\n",
    "# Random forest regressor\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=5,\n",
    "    max_features=\"sqrt\",\n",
    "    random_state=42,\n",
    ").fit(X_train_std, y_train)\n",
    "\n",
    "pred_rf_train = rf.predict(X_train_std)\n",
    "pred_rf_test = rf.predict(X_test_std)\n",
    "\n",
    "print(\"\\nRandomForestRegressor | n_estimators=300, max_depth=5, min_samples_leaf=5\")\n",
    "print(\"  train RMSE:\", rmse(y_train, pred_rf_train))\n",
    "print(\"  test  RMSE:\", rmse(y_test, pred_rf_test))\n",
    "print(\"  train R^2 :\", r2_score(y_train, pred_rf_train))\n",
    "print(\"  test  R^2 :\", r2_score(y_test, pred_rf_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781efe16",
   "metadata": {},
   "source": [
    "### 5.2 Sweep `max_depth` (forest)\n",
    "\n",
    "We choose `best_depth` by **minimum test RMSE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b461262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = list(range(1, 21))\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "train_r2 = []\n",
    "test_r2 = []\n",
    "\n",
    "for d in depths:\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=d,\n",
    "        min_samples_leaf=5,\n",
    "        max_features=\"sqrt\",\n",
    "        random_state=42,\n",
    "    ).fit(X_train_std, y_train)\n",
    "\n",
    "    pr_tr = model.predict(X_train_std)\n",
    "    pr_te = model.predict(X_test_std)\n",
    "\n",
    "    train_rmse.append(rmse(y_train, pr_tr))\n",
    "    test_rmse.append(rmse(y_test, pr_te))\n",
    "    train_r2.append(r2_score(y_train, pr_tr))\n",
    "    test_r2.append(r2_score(y_test, pr_te))\n",
    "\n",
    "best_d = depths[int(np.argmin(test_rmse))]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(depths, train_rmse, marker=\"o\", label=\"train RMSE\")\n",
    "plt.plot(depths, test_rmse, marker=\"o\", label=\"test RMSE\")\n",
    "plt.axvline(best_d, linestyle=\"--\", label=f\"best depth = {best_d}\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Diabetes — RMSE vs max_depth (Random Forest)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(depths, train_r2, marker=\"o\", label=\"train R^2\")\n",
    "plt.plot(depths, test_r2, marker=\"o\", label=\"test R^2\")\n",
    "plt.axvline(best_d, linestyle=\"--\", label=f\"best depth = {best_d}\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"R^2\")\n",
    "plt.title(\"Diabetes — R^2 vs max_depth (Random Forest)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Best depth by test RMSE:\", best_d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc4683",
   "metadata": {},
   "source": [
    "### 5.3 Sweep `min_samples_leaf` (forest, fixed depth)\n",
    "\n",
    "Larger leaf sizes regularize the forest and can improve generalization on noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f69b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves = [1, 2, 5, 10, 20, 40, 80]\n",
    "train_rmse_l = []\n",
    "test_rmse_l = []\n",
    "train_r2_l = []\n",
    "test_r2_l = []\n",
    "\n",
    "for leaf in leaves:\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=best_d,\n",
    "        min_samples_leaf=leaf,\n",
    "        max_features=\"sqrt\",\n",
    "        random_state=42,\n",
    "    ).fit(X_train_std, y_train)\n",
    "\n",
    "    pr_tr = model.predict(X_train_std)\n",
    "    pr_te = model.predict(X_test_std)\n",
    "\n",
    "    train_rmse_l.append(rmse(y_train, pr_tr))\n",
    "    test_rmse_l.append(rmse(y_test, pr_te))\n",
    "    train_r2_l.append(r2_score(y_train, pr_tr))\n",
    "    test_r2_l.append(r2_score(y_test, pr_te))\n",
    "\n",
    "best_leaf = leaves[int(np.argmin(test_rmse_l))]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(leaves, train_rmse_l, marker=\"o\", label=\"train RMSE\")\n",
    "plt.plot(leaves, test_rmse_l, marker=\"o\", label=\"test RMSE\")\n",
    "plt.axvline(best_leaf, linestyle=\"--\", label=f\"best leaf = {best_leaf}\")\n",
    "plt.xlabel(\"min_samples_leaf\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(f\"Diabetes — RMSE vs min_samples_leaf (max_depth={best_d})\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(leaves, train_r2_l, marker=\"o\", label=\"train R^2\")\n",
    "plt.plot(leaves, test_r2_l, marker=\"o\", label=\"test R^2\")\n",
    "plt.axvline(best_leaf, linestyle=\"--\", label=f\"best leaf = {best_leaf}\")\n",
    "plt.xlabel(\"min_samples_leaf\")\n",
    "plt.ylabel(\"R^2\")\n",
    "plt.title(f\"Diabetes — R^2 vs min_samples_leaf (max_depth={best_d})\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Best min_samples_leaf (with best_depth):\", best_leaf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f95badf",
   "metadata": {},
   "source": [
    "### 5.4 Compare `max_features` settings (forest)\n",
    "\n",
    "For regression forests, feature subsampling can still help by decorrelating trees, but the best choice is dataset-dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7192ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = [\n",
    "    (\"all (None)\", None),\n",
    "    (\"sqrt(p)\", \"sqrt\"),\n",
    "    (\"0.3*p\", 0.3),\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for name, mf in settings:\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=best_d,\n",
    "        min_samples_leaf=best_leaf,\n",
    "        max_features=mf,\n",
    "        random_state=42,\n",
    "    ).fit(X_train_std, y_train)\n",
    "\n",
    "    pr_tr = model.predict(X_train_std)\n",
    "    pr_te = model.predict(X_test_std)\n",
    "\n",
    "    rows.append([\n",
    "        name,\n",
    "        mf,\n",
    "        rmse(y_train, pr_tr),\n",
    "        rmse(y_test, pr_te),\n",
    "        r2_score(y_train, pr_tr),\n",
    "        r2_score(y_test, pr_te),\n",
    "    ])\n",
    "\n",
    "print(\"setting\\tmax_features\\ttrain_RMSE\\ttest_RMSE\\ttrain_R2\\ttest_R2\")\n",
    "for r in rows:\n",
    "    print(f\"{r[0]}\\t{r[1]}\\t{r[2]:.4f}\\t{r[3]:.4f}\\t{r[4]:.4f}\\t{r[5]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb3e260",
   "metadata": {},
   "source": [
    "## 6. Part D — 1D synthetic regression (visual intuition)\n",
    "\n",
    "Regression forests average many piecewise-constant trees, which often produces a **less jagged** function than a single deep tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43326627",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "n = 300\n",
    "x = rng.uniform(-3.0, 3.0, size=n)\n",
    "y = np.sin(x) + 0.15 * rng.normal(size=n)\n",
    "\n",
    "X1 = x.reshape(-1, 1)\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(\n",
    "    X1, y,\n",
    "    test_size=0.25,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X1_train_std, params1 = standardize(X1_train, return_params=True)\n",
    "mean1 = params1[\"mean\"]\n",
    "scale1 = params1[\"scale\"]\n",
    "X1_test_std = (X1_test - mean1) / scale1\n",
    "\n",
    "print(\"X1_train shape:\", X1_train.shape, \"X1_test shape:\", X1_test.shape)\n",
    "print(\"Train mean (approx):\", np.round(X1_train_std.mean(axis=0), 6))\n",
    "print(\"Train std  (approx):\", np.round(X1_train_std.std(axis=0), 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca36a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.linspace(X1_train_std.min() - 0.5, X1_train_std.max() + 0.5, 600).reshape(-1, 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X1_train_std[:, 0], y1_train, s=15, label=\"train\")\n",
    "plt.scatter(X1_test_std[:, 0], y1_test, s=15, label=\"test\")\n",
    "\n",
    "configs = [\n",
    "    (\"n=5\", 5),\n",
    "    (\"n=25\", 25),\n",
    "    (\"n=200\", 200),\n",
    "]\n",
    "for label, n_estimators in configs:\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=8,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=None,  # 1D\n",
    "        random_state=0,\n",
    "    ).fit(X1_train_std, y1_train)\n",
    "    plt.plot(grid[:, 0], model.predict(grid), label=label)\n",
    "\n",
    "plt.xlabel(\"x (standardized)\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"1D Synthetic — Random forest predictions (averaging piecewise-constant trees)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74413d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = list(range(1, 21))\n",
    "test_rmse = []\n",
    "train_rmse = []\n",
    "\n",
    "for d in depths:\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=d,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=None,\n",
    "        random_state=0,\n",
    "    ).fit(X1_train_std, y1_train)\n",
    "\n",
    "    train_rmse.append(rmse(y1_train, model.predict(X1_train_std)))\n",
    "    test_rmse.append(rmse(y1_test, model.predict(X1_test_std)))\n",
    "\n",
    "best_d_syn = depths[int(np.argmin(test_rmse))]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(depths, train_rmse, marker=\"o\", label=\"train RMSE\")\n",
    "plt.plot(depths, test_rmse, marker=\"o\", label=\"test RMSE\")\n",
    "plt.axvline(best_d_syn, linestyle=\"--\", label=f\"best depth = {best_d_syn}\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"1D Synthetic — RMSE vs max_depth (Random Forest)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Best depth by test RMSE (synthetic):\", best_d_syn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22353428",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "- **Bagging** (bootstrap + averaging) reduces variance: forests typically generalize better than a single deep tree.\n",
    "- Increasing **tree depth** drives training accuracy/R² up, but test performance often peaks at a moderate depth.\n",
    "- Increasing **`n_estimators`** usually stabilizes test performance (lower variance), with diminishing returns.\n",
    "- **Feature subsampling** (`max_features='sqrt'` / `'log2'` / fractions) is a key ingredient for random forests because it decorrelates trees.\n",
    "- Even though trees don’t require scaling, keeping a consistent **train-only preprocessing** step helps avoid leakage and makes experiments comparable across models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
