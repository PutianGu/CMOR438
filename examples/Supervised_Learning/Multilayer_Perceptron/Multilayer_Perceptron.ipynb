{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa298404",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP)\n",
    "\n",
    "This notebook demonstrates a **NumPy-only** implementation of a feed-forward neural network classifier:\n",
    "\n",
    "- `MultilayerPerceptronClassifier` (alias: `MLPClassifier`)\n",
    "- Hidden layers + nonlinearity (**ReLU** / **tanh**)\n",
    "- Training via backpropagation with **SGD** or **Adam**\n",
    "- Multiclass output with **softmax**\n",
    "\n",
    "We use scikit-learn **datasets only** (no scikit-learn models).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245b22e",
   "metadata": {},
   "source": [
    "## 1. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import inspect\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "\n",
    "# --- Ensure we can import from src/ ---\n",
    "here = Path.cwd()\n",
    "repo_root = None\n",
    "for p in [here] + list(here.parents):\n",
    "    if (p / \"src\" / \"rice_ml\").exists():\n",
    "        repo_root = p\n",
    "        break\n",
    "\n",
    "if repo_root is None:\n",
    "    raise RuntimeError(\"Could not find 'src/rice_ml'. Run this notebook inside the repo.\")\n",
    "\n",
    "sys.path.insert(0, str(repo_root / \"src\"))\n",
    "\n",
    "from rice_ml.processing.preprocessing import standardize, train_test_split\n",
    "from rice_ml.supervised_learning.multilayer_perceptron import MultilayerPerceptronClassifier\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b8fce1",
   "metadata": {},
   "source": [
    "## 2. Preprocessing wrapper (fits your repo API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23527c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_mean_scale(params):\n",
    "    \"\"\"Try to extract (mean, scale/std) from a variety of param formats.\"\"\"\n",
    "    if isinstance(params, dict):\n",
    "        mean = params.get(\"mean\", params.get(\"mu\", params.get(\"center\")))\n",
    "        scale = params.get(\"std\", params.get(\"sigma\", params.get(\"scale\")))\n",
    "        if scale is None and params.get(\"var\") is not None:\n",
    "            scale = np.sqrt(params[\"var\"])\n",
    "        return mean, scale\n",
    "\n",
    "    if isinstance(params, (tuple, list)) and len(params) == 2:\n",
    "        return params[0], params[1]\n",
    "\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def standardize_fit(X: np.ndarray):\n",
    "    \"\"\"Fit standardization on X and return (X_std, params).\"\"\"\n",
    "    out = standardize(X, return_params=True)\n",
    "    if not (isinstance(out, tuple) and len(out) == 2):\n",
    "        raise RuntimeError(\"Expected standardize(..., return_params=True) to return (X_std, params).\")\n",
    "    X_std, params = out\n",
    "    return X_std, params\n",
    "\n",
    "\n",
    "def standardize_apply(X: np.ndarray, params):\n",
    "    \"\"\"Apply a previously-fitted standardization to X.\"\"\"\n",
    "    sig = inspect.signature(standardize)\n",
    "    if \"params\" in sig.parameters:\n",
    "        return standardize(X, params=params)\n",
    "\n",
    "    # Fallback: apply manually\n",
    "    mean, scale = _extract_mean_scale(params)\n",
    "    if mean is None or scale is None:\n",
    "        raise RuntimeError(\n",
    "            \"Could not apply standardization: standardize() has no 'params' argument and params format was not recognized.\"\n",
    "        )\n",
    "\n",
    "    scale = np.asarray(scale, dtype=float)\n",
    "    mean = np.asarray(mean, dtype=float)\n",
    "    scale_safe = np.where(scale == 0.0, 1.0, scale)\n",
    "    return (X - mean) / scale_safe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d74d25",
   "metadata": {},
   "source": [
    "## 3. Helper metrics and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2626947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return float(np.mean(y_true == y_pred))\n",
    "\n",
    "\n",
    "def confusion_matrix_np(y_true: np.ndarray, y_pred: np.ndarray, labels=None) -> np.ndarray:\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    if labels is None:\n",
    "        labels = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    idx = {lab: i for i, lab in enumerate(labels)}\n",
    "    cm = np.zeros((labels.size, labels.size), dtype=int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        cm[idx[t], idx[p]] += 1\n",
    "    return cm\n",
    "\n",
    "\n",
    "def plot_decision_regions_2d(model, X: np.ndarray, y: np.ndarray, title: str) -> None:\n",
    "    \"\"\"Plot decision regions for a 2D dataset.\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 0.6, X[:, 0].max() + 0.6\n",
    "    y_min, y_max = X[:, 1].min() - 0.6, X[:, 1].max() + 0.6\n",
    "\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, 300),\n",
    "        np.linspace(y_min, y_max, 300),\n",
    "    )\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.predict(grid).reshape(xx.shape)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.contourf(xx, yy, Z, alpha=0.25)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\"k\", s=35)\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss_curve(clf, title: str) -> None:\n",
    "    if getattr(clf, \"loss_curve_\", None) is None or len(clf.loss_curve_) == 0:\n",
    "        print(\"No loss curve available.\")\n",
    "        return\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(1, len(clf.loss_curve_) + 1), clf.loss_curve_, marker=\"o\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"training loss (cross-entropy)\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5dbb99",
   "metadata": {},
   "source": [
    "## 4. Part A — XOR (nonlinear separability)\n",
    "\n",
    "A single linear classifier (e.g., perceptron / logistic regression) cannot solve XOR.\n",
    "A small MLP with one hidden layer **can** because it learns a nonlinear decision boundary.\n",
    "\n",
    "We standardize the inputs for training stability (not strictly required for XOR, but good practice).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273c1c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR dataset\n",
    "X = np.array([[0.0, 0.0],\n",
    "              [0.0, 1.0],\n",
    "              [1.0, 0.0],\n",
    "              [1.0, 1.0]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "X_std, params = standardize_fit(X)\n",
    "print(\"Train mean (approx):\", np.round(X_std.mean(axis=0), 4))\n",
    "print(\"Train std  (approx):\", np.round(X_std.std(axis=0), 4))\n",
    "\n",
    "clf_xor = MultilayerPerceptronClassifier(\n",
    "    hidden_layer_sizes=(8,),\n",
    "    activation=\"tanh\",\n",
    "    solver=\"adam\",\n",
    "    learning_rate=0.05,\n",
    "    batch_size=4,\n",
    "    max_iter=800,\n",
    "    random_state=0,\n",
    "    tol=1e-8,\n",
    "    n_iter_no_change=50,\n",
    "    early_stopping=False,\n",
    ").fit(X_std, y)\n",
    "\n",
    "pred = clf_xor.predict(X_std)\n",
    "print(\"Pred:\", pred)\n",
    "print(\"Acc :\", accuracy(y, pred))\n",
    "\n",
    "plot_loss_curve(clf_xor, \"XOR — training loss curve\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63e83f4",
   "metadata": {},
   "source": [
    "## 5. Part B — Iris (2D) multiclass classification + decision regions\n",
    "\n",
    "We take two Iris features so we can visualize the decision regions.  \n",
    "MLPs are **sensitive** to feature scaling, so we standardize using **train-only** statistics,\n",
    "then apply the same parameters to the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7adad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X_all = iris.data\n",
    "y_all = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Use 2D: petal length, petal width (classic separation)\n",
    "X2 = X_all[:, [2, 3]]\n",
    "y = y_all\n",
    "\n",
    "print(\"X2 shape:\", X2.shape)\n",
    "print(\"y shape :\", y.shape)\n",
    "print(\"classes :\", list(enumerate(iris.target_names)))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X2[:, 0], X2[:, 1], c=y, edgecolor=\"k\", s=35)\n",
    "plt.xlabel(feature_names[2])\n",
    "plt.ylabel(feature_names[3])\n",
    "plt.title(\"Iris (raw, 2D features)\")\n",
    "plt.show()\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X2, y, test_size=0.25, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize using TRAIN only (repo-compatible API)\n",
    "X_train_std, params = standardize_fit(X_train)\n",
    "X_test_std = standardize_apply(X_test, params)\n",
    "\n",
    "print(\"\\nTrain mean (approx):\", np.round(X_train_std.mean(axis=0), 4))\n",
    "print(\"Train std  (approx):\", np.round(X_train_std.std(axis=0), 4))\n",
    "print(\"Test  mean (approx):\", np.round(X_test_std.mean(axis=0), 4))\n",
    "print(\"Test  std  (approx):\", np.round(X_test_std.std(axis=0), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73716e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit baseline MLP (multiclass softmax)\n",
    "clf_iris = MultilayerPerceptronClassifier(\n",
    "    hidden_layer_sizes=(16,),\n",
    "    activation=\"tanh\",\n",
    "    solver=\"adam\",\n",
    "    learning_rate=0.01,\n",
    "    batch_size=16,\n",
    "    max_iter=600,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.15,\n",
    "    n_iter_no_change=20,\n",
    ").fit(X_train_std, y_train)\n",
    "\n",
    "tr_pred = clf_iris.predict(X_train_std)\n",
    "te_pred = clf_iris.predict(X_test_std)\n",
    "\n",
    "print(\"MLPClassifier (Iris 2D)\")\n",
    "print(\"  train acc:\", accuracy(y_train, tr_pred))\n",
    "print(\"  test  acc:\", accuracy(y_test, te_pred))\n",
    "print(\"\\nConfusion matrix (test): rows=true, cols=pred, labels=[0,1,2]\")\n",
    "print(confusion_matrix_np(y_test, te_pred, labels=np.array([0, 1, 2])))\n",
    "\n",
    "plot_loss_curve(clf_iris, \"Iris 2D — training loss curve\")\n",
    "plot_decision_regions_2d(clf_iris, X_train_std, y_train, \"Decision regions (train) — Iris 2D | MLP\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b7c34",
   "metadata": {},
   "source": [
    "### 5.1 Hyperparameter sweeps (small grid)\n",
    "\n",
    "We scan a few reasonable values to show typical effects:\n",
    "\n",
    "- More hidden units can increase capacity (but may overfit)\n",
    "- Learning rate controls optimization speed/stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c8b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [4, 8, 16, 32]\n",
    "lrs = [0.003, 0.01, 0.03]\n",
    "\n",
    "results = []\n",
    "for h in hidden_sizes:\n",
    "    for lr in lrs:\n",
    "        clf = MultilayerPerceptronClassifier(\n",
    "            hidden_layer_sizes=(h,),\n",
    "            activation=\"tanh\",\n",
    "            solver=\"adam\",\n",
    "            learning_rate=lr,\n",
    "            batch_size=16,\n",
    "            max_iter=500,\n",
    "            random_state=0,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.15,\n",
    "            n_iter_no_change=15,\n",
    "        ).fit(X_train_std, y_train)\n",
    "\n",
    "        tr = accuracy(y_train, clf.predict(X_train_std))\n",
    "        te = accuracy(y_test, clf.predict(X_test_std))\n",
    "        results.append((h, lr, tr, te))\n",
    "\n",
    "best = max(results, key=lambda t: t[3])\n",
    "print(\"Best (by test acc): hidden =\", best[0], \"lr =\", best[1], \"train acc =\", best[2], \"test acc =\", best[3])\n",
    "\n",
    "plt.figure()\n",
    "for lr in lrs:\n",
    "    xs = [h2 for (h2, lr2, tr, te) in results if lr2 == lr]\n",
    "    ys = [te for (h2, lr2, tr, te) in results if lr2 == lr]\n",
    "    plt.plot(xs, ys, marker=\"o\", label=f\"lr={lr}\")\n",
    "plt.xlabel(\"hidden_layer_size\")\n",
    "plt.ylabel(\"test accuracy\")\n",
    "plt.title(\"Iris 2D — test accuracy vs hidden size\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea7b32",
   "metadata": {},
   "source": [
    "## 6. Part C — Breast Cancer (30D) binary classification\n",
    "\n",
    "Here we use all 30 features. Standardization is important for stable MLP training.\n",
    "We also compare **SGD vs Adam** briefly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bbc00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target  # 0/1\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"classes:\", list(enumerate(data.target_names)))\n",
    "print(\"class counts:\", {int(k): int(v) for k, v in zip(*np.unique(y, return_counts=True))})\n",
    "print(\"Any NaN in X?\", bool(np.isnan(X).any()))\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize using TRAIN only (repo-compatible API)\n",
    "X_train_std, params = standardize_fit(X_train)\n",
    "X_test_std = standardize_apply(X_test, params)\n",
    "\n",
    "print(\"\\nTrain mean (approx):\", np.round(X_train_std.mean(axis=0)[:5], 4))\n",
    "print(\"Train std  (approx):\", np.round(X_train_std.std(axis=0)[:5], 4))\n",
    "print(\"Test  mean (approx):\", np.round(X_test_std.mean(axis=0)[:5], 4))\n",
    "print(\"Test  std  (approx):\", np.round(X_test_std.std(axis=0)[:5], 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3009889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam\n",
    "clf_adam = MultilayerPerceptronClassifier(\n",
    "    hidden_layer_sizes=(32, 16),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    learning_rate=0.01,\n",
    "    batch_size=32,\n",
    "    max_iter=400,\n",
    "    random_state=42,\n",
    "    alpha=1e-4,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.15,\n",
    "    n_iter_no_change=20,\n",
    ").fit(X_train_std, y_train)\n",
    "\n",
    "tr_pred = clf_adam.predict(X_train_std)\n",
    "te_pred = clf_adam.predict(X_test_std)\n",
    "\n",
    "print(\"MLP (Adam) — Breast Cancer\")\n",
    "print(\"  train acc:\", accuracy(y_train, tr_pred))\n",
    "print(\"  test  acc:\", accuracy(y_test, te_pred))\n",
    "print(\"\\nConfusion matrix (test): rows=true, cols=pred, labels=[0,1]\")\n",
    "print(confusion_matrix_np(y_test, te_pred, labels=np.array([0, 1])))\n",
    "\n",
    "plot_loss_curve(clf_adam, \"Breast Cancer — training loss curve (Adam)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b9b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD (typically needs smaller learning rate / more epochs)\n",
    "clf_sgd = MultilayerPerceptronClassifier(\n",
    "    hidden_layer_sizes=(32, 16),\n",
    "    activation=\"relu\",\n",
    "    solver=\"sgd\",\n",
    "    learning_rate=0.005,\n",
    "    batch_size=32,\n",
    "    max_iter=600,\n",
    "    random_state=42,\n",
    "    alpha=1e-4,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.15,\n",
    "    n_iter_no_change=25,\n",
    ").fit(X_train_std, y_train)\n",
    "\n",
    "tr_pred = clf_sgd.predict(X_train_std)\n",
    "te_pred = clf_sgd.predict(X_test_std)\n",
    "\n",
    "print(\"MLP (SGD) — Breast Cancer\")\n",
    "print(\"  train acc:\", accuracy(y_train, tr_pred))\n",
    "print(\"  test  acc:\", accuracy(y_test, te_pred))\n",
    "\n",
    "plot_loss_curve(clf_sgd, \"Breast Cancer — training loss curve (SGD)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a9c90f",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "- MLPs combine **linear layers + nonlinear activations** to learn complex decision boundaries (e.g., XOR).\n",
    "- Unlike trees, MLPs are **sensitive to feature scaling**; standardization improves training stability and speed.\n",
    "- **Adam** generally converges faster and is more forgiving than plain SGD, especially early in training.\n",
    "- Capacity control matters: hidden size/depth and regularization (e.g., `alpha`, early stopping) help avoid overfitting.\n",
    "\n",
    "**Implementation note:** this notebook uses a small wrapper (`standardize_fit` / `standardize_apply`) so it works with\n",
    "your repository's `standardize()` API (which returns `(X_std, params)` and may or may not accept `params=` when applying\n",
    "to test data).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
