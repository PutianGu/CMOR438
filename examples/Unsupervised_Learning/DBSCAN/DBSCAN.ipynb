{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56088fea",
   "metadata": {},
   "source": [
    "# DBSCAN\n",
    "\n",
    "This notebook demonstrates how to use **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) from the `rice_ml.unsupervised_learning` package, and compares its behavior with **K-Means** on the same dataset.\n",
    "\n",
    "DBSCAN groups points that lie in dense regions and labels low-density points as **noise** (`-1`). Unlike K-Means, it does **not** require choosing `k` in advance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc10a5c0",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "We first import the required modules. When running this notebook directly from the `examples/` folder, we also add the repository's `src/` directory to `sys.path` so that `rice_ml` can be imported without installing the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d1e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Make `rice_ml` importable when running the notebook directly ---\n",
    "cwd = Path.cwd().resolve()\n",
    "for p in [cwd] + list(cwd.parents):\n",
    "    if (p / \"src\" / \"rice_ml\").exists():\n",
    "        sys.path.insert(0, str(p / \"src\"))\n",
    "        break\n",
    "else:\n",
    "    raise RuntimeError(\"Could not find 'src/rice_ml'. Run this notebook inside the repo, or install the package.\")\n",
    "\n",
    "from rice_ml.processing.preprocessing import standardize\n",
    "from rice_ml.unsupervised_learning.dbscan import dbscan\n",
    "from rice_ml.unsupervised_learning.k_means_clustering import kmeans\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d8e51e",
   "metadata": {},
   "source": [
    "## 2. Load a dataset\n",
    "\n",
    "For a simple and common clustering demo, we use the **Iris** dataset.\n",
    "\n",
    "- It has **150** samples and **4** numeric features.\n",
    "- It includes a ground-truth species label, which we will use only for **qualitative comparison** and an optional clustering score (Adjusted Rand Index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15f3bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris is a classic toy dataset; scikit-learn is OK to use inside notebooks.\n",
    "try:\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.metrics import adjusted_rand_score\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        \"This notebook uses scikit-learn for the Iris dataset and an optional ARI metric. \"\n",
    "        \"Install it with: pip install scikit-learn\"\n",
    "    ) from e\n",
    "\n",
    "iris = load_iris()\n",
    "X_raw = iris.data\n",
    "y_true = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "print(\"X shape:\", X_raw.shape)\n",
    "print(\"Classes:\", np.unique(y_true), \"(n_classes =\", len(np.unique(y_true)), \")\")\n",
    "print(\"Feature names:\", feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c96f17",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "\n",
    "Both DBSCAN and K-Means rely on distances. To avoid one feature dominating due to scale, we **standardize** each feature:\n",
    "\n",
    "\\[ X_{std} = \\frac{X - \\mu}{\\sigma} \\]\n",
    "\n",
    "We use the `standardize` helper from `rice_ml.processing.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9de971",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = standardize(X_raw)\n",
    "\n",
    "# For visualization, we will use the last two features (petal length/width),\n",
    "# which usually separate the Iris species better than the first two.\n",
    "X_vis = X[:, 2:4]\n",
    "\n",
    "print(\"Standardized X mean (approx):\", np.round(X.mean(axis=0), 3))\n",
    "print(\"Standardized X std (approx):\", np.round(X.std(axis=0), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a661361",
   "metadata": {},
   "source": [
    "## 4. Helper: 2D scatter plot\n",
    "\n",
    "To keep plots readable, we visualize clusters in 2D using the standardized petal features (`X[:, 2:4]`).\n",
    "\n",
    "Noise points (label `-1`) are drawn with a different marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9180b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters_2d(X2, labels, title):\n",
    "    labels = np.asarray(labels)\n",
    "    unique = np.unique(labels)\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "\n",
    "    # Plot clusters (including noise)\n",
    "    for lab in unique:\n",
    "        mask = labels == lab\n",
    "        if lab == -1:\n",
    "            plt.scatter(X2[mask, 0], X2[mask, 1], marker=\"x\", s=50, label=\"noise (-1)\")\n",
    "        else:\n",
    "            plt.scatter(X2[mask, 0], X2[mask, 1], s=40, label=f\"cluster {int(lab)}\")\n",
    "\n",
    "    plt.xlabel(\"petal length (standardized)\")\n",
    "    plt.ylabel(\"petal width (standardized)\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"best\", fontsize=9)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a215ef",
   "metadata": {},
   "source": [
    "## 5. Run DBSCAN (rice_ml)\n",
    "\n",
    "Key parameters:\n",
    "\n",
    "- `eps`: neighborhood radius for density (larger `eps` → more points become connected)\n",
    "- `min_samples`: minimum neighbors to form a **core** point (larger `min_samples` → stricter density)\n",
    "\n",
    "`dbscan` returns:\n",
    "\n",
    "- `labels_db`: array of cluster labels (`-1` means noise)\n",
    "- `n_clusters_db`: number of clusters (excluding noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0de61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.6\n",
    "min_samples = 6\n",
    "\n",
    "labels_db, n_clusters_db = dbscan(X, eps=eps, min_samples=min_samples)\n",
    "\n",
    "n_noise = int(np.sum(labels_db == -1))\n",
    "print(\"DBSCAN params: eps =\", eps, \", min_samples =\", min_samples)\n",
    "print(\"Clusters found (excluding noise):\", n_clusters_db)\n",
    "print(\"Noise points:\", n_noise, f\"({n_noise / len(labels_db):.1%})\")\n",
    "\n",
    "plot_clusters_2d(X_vis, labels_db, f\"DBSCAN (rice_ml): eps={eps}, min_samples={min_samples}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc60f8",
   "metadata": {},
   "source": [
    "## 6. Compare with K-Means (rice_ml)\n",
    "\n",
    "K-Means always produces exactly `k` clusters and assigns **every** point to some cluster.\n",
    "\n",
    "Here we set `k=3` because Iris has three species (again: this is just for comparison; K-Means is still unsupervised)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6a6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "labels_km, centers_km = kmeans(X, k=k)\n",
    "\n",
    "print(\"K-Means: k =\", k)\n",
    "print(\"Cluster sizes:\", np.bincount(labels_km))\n",
    "\n",
    "plot_clusters_2d(X_vis, labels_km, \"K-Means (rice_ml): k=3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799c8a8",
   "metadata": {},
   "source": [
    "## 7. Compare against ground truth labels\n",
    "\n",
    "Clustering is unsupervised, so there is no single “correct” output. But since Iris provides true species labels, we can compute an external clustering metric:\n",
    "\n",
    "- **Adjusted Rand Index (ARI)**: 1.0 means perfect match, ~0 means random assignment.\n",
    "\n",
    "For DBSCAN, we exclude noise points from the ARI calculation (otherwise noise penalizes heavily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8877e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_km = adjusted_rand_score(y_true, labels_km)\n",
    "\n",
    "# Exclude noise for DBSCAN ARI\n",
    "mask = labels_db != -1\n",
    "if mask.sum() > 0:\n",
    "    ari_db = adjusted_rand_score(y_true[mask], labels_db[mask])\n",
    "else:\n",
    "    ari_db = np.nan\n",
    "\n",
    "print(\"ARI (K-Means vs true labels):\", round(float(ari_km), 4))\n",
    "print(\"ARI (DBSCAN vs true labels, excluding noise):\", (None if np.isnan(ari_db) else round(float(ari_db), 4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff8469d",
   "metadata": {},
   "source": [
    "## 8. Interpretation\n",
    "\n",
    "- DBSCAN can find clusters of **arbitrary shape** and can label ambiguous points as **noise**.\n",
    "- K-Means forces exactly `k` **compact** clusters and assigns every point to a cluster.\n",
    "\n",
    "On Iris, K-Means often produces a reasonable 3-way partition, while DBSCAN’s result can be more sensitive to `eps` and `min_samples` (because the Iris species overlap in feature space).\n",
    "\n",
    "**Try exploring (recommended):**\n",
    "- Increase `eps` to merge nearby dense regions (fewer clusters, less noise).\n",
    "- Decrease `eps` to split clusters or create more noise points.\n",
    "- Increase `min_samples` to demand denser clusters (more noise, fewer clusters).\n",
    "\n",
    "In practice, DBSCAN is often strongest when you expect **non-spherical** clusters and meaningful **outliers/noise**, while K-Means is strong when clusters are roughly **spherical** and well-separated.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
